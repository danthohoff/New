import polars as pl

def interactions_nrw(
    df_zlm: pl.DataFrame,
    nrw_bsts: list[str],
    *,
    window_minutes: int = 10,
    mode: str = "bucket_combinatorics",   # "bucket_combinatorics" | "unique_per_day"
    split_nrw_state: bool = True,         # in beiden Modi verfügbar (robust via first_nrw_time)
    group_by_day: bool = False,           # True → je Betriebstag zurückgeben
    include_checks: bool = False,         # Konsistenz-Spalten (_sum_chk_*) mit ausgeben
) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Einheitliche Interaktionszählung NRW vs. Nicht-NRW an Betriebsstellen.

    Erwartete Spalten in df_zlm:
      - "Betriebstag" (date-like)
      - "zn"          (Zugnummer; string/zahl)
      - "bst"         (Betriebsstelle; string)
      - "istzeit_dt"  (Datetime)

    Parameter
    ---------
    window_minutes : Länge des disjunkten Zeit-Buckets (beide Modi).
    mode :
      - "bucket_combinatorics": disjunkte Zeit-Buckets; Kombinatorik je Bucket (zeitgewichtete Kontakte).
      - "unique_per_day": je Tag/BST wird jedes Zugpaar max. 1× gezählt (dauerunabhängig).
    split_nrw_state :
      - True: „gemischt“ zusätzlich aufteilen in „kommt aus NRW“ (fromNRW) und „fährt noch nach NRW“ (enterLater).
        Die Partitionierung basiert robust auf first_nrw_time.
    group_by_day :
      - True: Ergebnis pro (Betriebstag, bst). False: über den Zeitraum summiert pro bst.
    include_checks :
      - True: Diagnosespalten ausgeben, die Konsistenzen prüfen (sollten = 0 sein).

    Rückgabe
    --------
    per_bst : Polars-DataFrame mit Spalten:
      - inter_NRWs, inter_NichtNRWs, inter_gemischt
      - (optional) inter_gemischt_fromNRW, inter_gemischt_enterLater
      - (optional) _sum_chk_part, _sum_chk_mix (bei include_checks=True)
    debug_df : Polars-DataFrame (Buckets & Counts bei bucket_combinatorics, bzw. Paarliste bei unique_per_day)
    """

    # 0) Basis + NRW-Flag + disjunkter Bucket
    ev = (
        df_zlm
        .select(["Betriebstag", "zn", "bst", "istzeit_dt"])
        .with_columns([
            pl.col("bst").is_in(nrw_bsts).alias("in_nrw"),
        ])
        .sort(["Betriebstag", "zn", "istzeit_dt"])
        .with_columns([
            pl.col("in_nrw").any().over(["Betriebstag","zn"]).alias("is_nrw_train"),
            pl.col("istzeit_dt").dt.truncate(f"{window_minutes}m").alias("t_bucket"),
        ])
    )

    # Robust: first_nrw_time (erster NRW-Zeitpunkt je (Tag, Zug))
    first_nrw = (
        ev.filter(pl.col("in_nrw"))
          .group_by(["Betriebstag","zn"])
          .agg(pl.col("istzeit_dt").min().alias("first_nrw_time"))
    )
    # ev mit first_nrw_time verbinden
    ev = ev.join(first_nrw, on=["Betriebstag","zn"], how="left")

    # --------------------------------------------
    # MODE 1: BUCKET COMBINATORICS (zeitgewichtet)
    # --------------------------------------------
    if mode == "bucket_combinatorics":
        # Dedupe: jeder Zug pro (Tag, BST, Bucket) nur einmal zählen (wichtig für Y-BST etc.)
        ev_dedup = (
            ev.unique(subset=["Betriebstag","zn","bst","t_bucket"])
              .with_columns([
                  # robuste Partitionierung (immer disjunkt & vollständig, wenn is_nrw_train=True)
                  (pl.col("is_nrw_train") & (pl.col("istzeit_dt") <  pl.col("first_nrw_time"))).fill_null(False).alias("enter_flag"),
                  (pl.col("is_nrw_train") & (pl.col("istzeit_dt") >= pl.col("first_nrw_time"))).fill_null(False).alias("from_flag"),
              ])
        )

        # Zählungen je (Tag, BST, Bucket)
        counts = (
            ev_dedup.group_by(["Betriebstag","bst","t_bucket"])
            .agg([
                pl.col("is_nrw_train").sum().cast(pl.Int64).alias("n_nrw"),
                (~pl.col("is_nrw_train")).sum().cast(pl.Int64).alias("n_non"),
                pl.col("from_flag").sum().cast(pl.Int64).alias("n_from"),
                pl.col("enter_flag").sum().cast(pl.Int64).alias("n_enter"),
            ])
        )

        # Diagnose: Partition sollte exakt n_nrw aufteilen
        counts = counts.with_columns([
            (pl.col("n_from") + pl.col("n_enter") - pl.col("n_nrw")).alias("_chk_part")
        ])

        # Kombinatorik je Bucket (ohne Paarmaterialisierung)
        counts = counts.with_columns([
            (pl.col("n_nrw") * (pl.col("n_nrw") - 1) // 2).alias("c_NRWs"),
            (pl.col("n_non") * (pl.col("n_non") - 1) // 2).alias("c_NichtNRWs"),
            (pl.col("n_nrw") * pl.col("n_non")).alias("c_gemischt"),
        ])

        if split_nrw_state:
            counts = counts.with_columns([
                (pl.col("n_from")  * pl.col("n_non")).alias("c_gemischt_fromNRW"),
                (pl.col("n_enter") * pl.col("n_non")).alias("c_gemischt_enterLater"),
                # Summe der Splits sollte exakt c_gemischt ergeben
                (pl.col("c_gemischt_fromNRW") + pl.col("c_gemischt_enterLater") - pl.col("c_gemischt")).alias("_chk_mix"),
            ])
        else:
            counts = counts.with_columns([
                pl.lit(0).alias("c_gemischt_fromNRW"),
                pl.lit(0).alias("c_gemischt_enterLater"),
                pl.lit(0).alias("_chk_mix"),
            ])

        # Aggregation
        group_keys = ["bst"] if not group_by_day else ["Betriebstag","bst"]
        agg_cols = [
            pl.sum("c_NRWs").alias("inter_NRWs"),
            pl.sum("c_NichtNRWs").alias("inter_NichtNRWs"),
            pl.sum("c_gemischt").alias("inter_gemischt"),
        ]
        if split_nrw_state:
            agg_cols += [
                pl.sum("c_gemischt_fromNRW").alias("inter_gemischt_fromNRW"),
                pl.sum("c_gemischt_enterLater").alias("inter_gemischt_enterLater"),
            ]
        if include_checks:
            agg_cols += [
                pl.sum("_chk_part").alias("_sum_chk_part"),
                pl.sum("_chk_mix").alias("_sum_chk_mix"),
            ]

        per_bst = counts.group_by(group_keys).agg(agg_cols).sort(group_keys)
        debug_df = counts
        return per_bst, debug_df

    # --------------------------------------------
    # MODE 2: UNIQUE PER DAY (jede Paarung 1×/Tag/BST)
    # --------------------------------------------
    elif mode == "unique_per_day":
        # Wir nutzen disjunkte Buckets, um Co-Location festzustellen,
        # deduplizieren Paare dann aber je (Tag, BST).
        ev_binned = ev.select(["Betriebstag","zn","bst","t_bucket","istzeit_dt","is_nrw_train","first_nrw_time"]).unique()

        # repräsentative Zeit pro (Tag, Zug, BST, Bucket): min(istzeit_dt)
        ev_time = (
            ev_binned.group_by(["Betriebstag","zn","bst","t_bucket"])
                     .agg(pl.col("istzeit_dt").min().alias("approx_time"))
        )

        # Paare innerhalb des gleichen Buckets (Tag, BST, t_bucket)
        pairs_bucket = (
            ev_time.join(ev_time, on=["Betriebstag","bst","t_bucket"], how="inner", suffix="_r")
                   .filter(pl.col("zn") < pl.col("zn_r"))
                   .select(["Betriebstag","bst","zn","zn_r","approx_time","approx_time_r"])
        )

        # 1× pro Tag/BST deduplizieren
        pairs_unique = pairs_bucket.unique(subset=["Betriebstag","bst","zn","zn_r"])

        # NRW-Infos je Zug (ist NRW-Zug? und first_nrw_time)
        nrw_info = (
            ev.select(["Betriebstag","zn","is_nrw_train","first_nrw_time"]).unique(subset=["Betriebstag","zn"])
        )

        pairs_labeled = (
            pairs_unique
            .join(nrw_info, on=["Betriebstag","zn"], how="left")
            .rename({"is_nrw_train":"is_nrw_left","first_nrw_time":"first_left"})
            .join(nrw_info, left_on=["Betriebstag","zn_r"], right_on=["Betriebstag","zn"], how="left")
            .rename({"is_nrw_train":"is_nrw_right","first_nrw_time":"first_right"})
            .with_columns([
                (pl.col("is_nrw_left") & pl.col("is_nrw_right")).alias("pair_nrw_nrw"),
                (~pl.col("is_nrw_left") & ~pl.col("is_nrw_right")).alias("pair_non_non"),
                (pl.col("is_nrw_left") ^ pl.col("is_nrw_right")).alias("pair_mixed"),
            ])
        )

        if split_nrw_state:
            # Aufteilung gemischt anhand first_nrw_time vs. Begegnungszeit
            pairs_labeled = pairs_labeled.with_columns([
                (
                    (pl.col("is_nrw_left") & ~pl.col("is_nrw_right") & (pl.col("approx_time")   >= pl.col("first_left")))
                    | (pl.col("is_nrw_right") & ~pl.col("is_nrw_left") & (pl.col("approx_time_r") >= pl.col("first_right")))
                ).fill_null(False).alias("pair_mixed_fromNRW"),
                (
                    (pl.col("is_nrw_left") & ~pl.col("is_nrw_right") & (pl.col("approx_time")   <  pl.col("first_left")))
                    | (pl.col("is_nrw_right") & ~pl.col("is_nrw_left") & (pl.col("approx_time_r") <  pl.col("first_right")))
                ).fill_null(False).alias("pair_mixed_enterLater"),
            ])
        else:
            pairs_labeled = pairs_labeled.with_columns([
                pl.lit(False).alias("pair_mixed_fromNRW"),
                pl.lit(False).alias("pair_mixed_enterLater"),
            ])

        # Aggregation
        group_keys = ["bst"] if not group_by_day else ["Betriebstag","bst"]
        agg_cols = [
            pl.sum("pair_nrw_nrw").alias("inter_NRWs"),
            pl.sum("pair_non_non").alias("inter_NichtNRWs"),
            pl.sum("pair_mixed").alias("inter_gemischt"),
        ]
        if split_nrw_state:
            agg_cols += [
                pl.sum("pair_mixed_fromNRW").alias("inter_gemischt_fromNRW"),
                pl.sum("pair_mixed_enterLater").alias("inter_gemischt_enterLater"),
            ]

        per_bst = pairs_labeled.group_by(group_keys).agg(agg_cols).sort(group_keys)

        if include_checks:
            # Prüfe, ob Splits Summe ergeben (muss hier nicht exakt sein, falls first_* null)
            per_bst = per_bst.with_columns([
                (pl.col("inter_gemischt_fromNRW").fill_null(0) + pl.col("inter_gemischt_enterLater").fill_null(0) - pl.col("inter_gemischt").fill_null(0)).alias("_sum_chk_mix")
            ])

        debug_df = pairs_labeled
        return per_bst, debug_df

    else:
        raise ValueError("mode muss 'bucket_combinatorics' oder 'unique_per_day' sein.")