import re
import datetime as dt
import polars as pl
from loguru import logger
from pathlib import Path

# --- optional: normalize-Funktionen, falls deine ZN mal Sonderzeichen/Suffixe haben ---
def _normalize_zn_expr(colname: str) -> pl.Expr:
    # Beispiel: alles außer Ziffern entfernen; passe das bei Bedarf an!
    return (pl.col(colname)
            .cast(pl.Utf8)
            .str.strip()
            .str.replace_all(r"\D+", ""))

def load_line_punctuality(
    config: dict,
    tage: list[dt.date],
    mapping_csv: str | Path,
    *,
    zn_column_in_zlm: str = "zn",         # Spaltenname in zlm
    zn_column_in_map: str = "Zugnummer",  # Spaltenname in Zuordnungs-CSV
    line_column: str = "Nr",              # Spaltenname Linie in Zuordnungs-CSV
    ontime_threshold_sec: int = 360,
    separator_zlm: str = ":",             # ";" falls bei euch Semikolon
    decimal_comma: bool = True,           # False falls Punkt als Dezimal
    cache_tag: str = "v_line_punct"
) -> pl.DataFrame:
    """
    Ergebnis: DataFrame mit Spalten:
      Betriebstag | Linie (Nr) | Halte | pü Halte | PÜ-Quote | Median vsp | N Züge (distinct) | KW
    """

    produkt = config["produkt"]
    data_root = Path(config["path_to_SQF_preprocessed"])
    cache_dir = Path(config["path_to_data"])
    cache_dir.mkdir(parents=True, exist_ok=True)

    # ---- Cache ---------------------------------------------------------------
    cfg_key = f"{cache_tag}_{produkt}_{min(tage):%y%m%d}_{max(tage):%y%m%d}"
    cache_file = cache_dir / f"{cfg_key}.csv"
    if cache_file.exists():
        logger.info(f"Lese Linien-Pünktlichkeit aus Cache: {cache_file}")
        df = pl.read_csv(cache_file, try_parse_dates=True)
        if "KW" not in df.columns and "Betriebstag" in df.columns:
            df = df.with_columns(pl.col("Betriebstag").strptime(pl.Date, fmt="%Y-%m-%d", strict=False).dt.week().alias("KW"))
        return df

    # ---- Mapping lesen -------------------------------------------------------
    map_df = (
        pl.read_csv(mapping_csv)
        .select([zn_column_in_map, line_column])
        .with_columns([
            _normalize_zn_expr(zn_column_in_map).alias("_ZN_MAP"),
            pl.col(line_column).cast(pl.Utf8).alias("Linie"),
        ])
        .select(["_ZN_MAP", "Linie"])
        .unique()
    )
    if map_df.height == 0:
        raise ValueError("Zuordnungs-CSV leer oder Spaltennamen stimmen nicht.")

    # ---- zlm-Dateien einsammeln ---------------------------------------------
    files = [
        p for d in tage
        for p in (data_root / "zlm_lean" / produkt).glob(f"*_{d:%Y%m%d}_*.csv")
    ]
    if not files:
        raise ValueError("Keine zlm-Dateien im Zeitraum gefunden.")
    logger.info(f"{len(files)} zlm-Dateien werden verarbeitet …")

    def read_zlm_one(path: Path) -> pl.DataFrame:
        df = pl.read_csv(
            path,
            separator=separator_zlm,
            decimal_comma=decimal_comma,
            infer_schema_length=2000
        )
        # nur benötigte Spalten
        need = [ "istzeit", zn_column_in_zlm, "fsStatus", "vsp" ]
        cols = [c for c in need if c in df.columns]
        df = df.select(cols)

        casts = []
        if "istzeit" in df.columns:
            casts.append(pl.col("istzeit").str.strptime(pl.Datetime, format="%Y-%m-%d %H:%M:%S", strict=False).alias("istzeit_dt"))
        if zn_column_in_zlm in df.columns:
            casts.append(pl.col(zn_column_in_zlm).cast(pl.Utf8).alias("zn_raw"))
        if "fsStatus" in df.columns:
            casts.append(pl.col("fsStatus").cast(pl.Int64, strict=False))
        if "vsp" in df.columns:
            casts.append(pl.col("vsp").cast(pl.Int64, strict=False))
        df = df.with_columns(casts)

        # Betriebstag stabil aus erstem Datum im Dateinamen
        dates = re.findall(r"(\d{8})", str(path))
        if not dates:
            raise ValueError(f"Kann Datum nicht aus Dateinamen parsen: {path}")
        s = dates[0]  # nur das erste Datum = Betriebstag
        btag = dt.date(int(s[0:4]), int(s[4:6]), int(s[6:8]))
        return (df
                .with_columns([
                    pl.lit(btag).alias("Betriebstag"),
                    _normalize_zn_expr("zn_raw").alias("_ZN"),
                ])
               )

    df_zlm = pl.concat([read_zlm_one(p) for p in files], how="vertical_relaxed")

    # ---- Halte filtern & PÜ-Flag --------------------------------------------
    df_halte = (
        df_zlm
        .filter(pl.col("fsStatus").is_in([1, 3]))
        .with_columns((pl.col("vsp") < ontime_threshold_sec).alias("puenktlich"))
        .select(["Betriebstag", "_ZN", "istzeit_dt", "vsp", "puenktlich"])
    )

    # ---- Linien-Zuordnung joinen --------------------------------------------
    df_line = (
        df_halte
        .join(map_df, left_on="_ZN", right_on="_ZN_MAP", how="inner")
        .select(["Betriebstag", "Linie", "_ZN", "istzeit_dt", "vsp", "puenktlich"])
    )

    # ---- Aggregation pro Tag & Linie ----------------------------------------
    per_day_line = (
        df_line
        .group_by(["Betriebstag", "Linie"])
        .agg([
            pl.len().alias("Halte"),
            pl.col("puenktlich").sum().cast(pl.Int64).alias("pü Halte"),
            (pl.col("puenktlich").sum() / pl.len()).alias("PÜ-Quote"),
            pl.col("vsp").median().alias("Median vsp"),
            pl.col("_ZN").n_unique().alias("N Züge (distinct)"),
        ])
        .with_columns(pl.col("Betriebstag").dt.week().alias("KW"))
        .sort(["Betriebstag", "Linie"])
    )

    # optional: fehlende Tage auffüllen pro Linie kannst du später per outer merge mit einem Kalendarium machen

    # ---- Cache schreiben & zurück -------------------------------------------
    per_day_line.write_csv(cache_file)
    logger.success(f"Linien-Pünktlichkeit geschrieben: {cache_file}")
    return per_day_line