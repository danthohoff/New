import polars as pl

def interactions_nrw_disjoint_buckets(
    df_zlm: pl.DataFrame,
    nrw_bsts: list[str],
    window_minutes: int = 10,
    split_nrw_state: bool = True,
) -> pl.DataFrame:
    """
    Disjunkte Zeitfenster (Bucket = window_minutes) pro (Betriebstag, bst).
    In jedem Bucket Kombinatorik statt Paarbildung:
      NRW↔NRW = n_nrw*(n_nrw-1)/2
      Non↔Non = n_non*(n_non-1)/2
      NRW↔Non = n_nrw*n_non
    Optionaler Split für gemischte Kontakte (fromNRW / enterLater).
    Gibt je bst die aufsummierten Interaktionen zurück.
    """
    # 1) Basis + NRW-Flags + disjunkter Bucket
    ev = (
        df_zlm
        .select(["Betriebstag","zn","bst","istzeit_dt"])
        .with_columns(pl.col("bst").is_in(nrw_bsts).alias("in_nrw"))
        .sort(["Betriebstag","zn","istzeit_dt"])
    )

    # past/future nur nötig, wenn Split gewünscht
    if split_nrw_state:
        ev = ev.with_columns([
            pl.col("in_nrw").cum_max().over(["Betriebstag","zn"]).shift(1).fill_null(False).alias("past_in_nrw"),
            pl.col("in_nrw").reverse().cum_max().reverse().over(["Betriebstag","zn"]).shift(-1).fill_null(False).alias("future_in_nrw"),
        ])

    ev = ev.with_columns([
        pl.col("in_nrw").any().over(["Betriebstag","zn"]).alias("is_nrw_train"),
        pl.when(pl.col("in_nrw").any().over(["Betriebstag","zn"]))
          .then(pl.lit("NRW")).otherwise(pl.lit("Nicht-NRW")).alias("train_type"),
        # disjunkter Bucket mit Länge window_minutes
        pl.col("istzeit_dt").dt.truncate(f"{window_minutes}m").alias("t_bucket"),
    ])

    # 2) Zählungen pro (Tag, bst, Bucket)
    base_counts = (
        ev.group_by(["Betriebstag","bst","t_bucket"])
          .agg([
              pl.col("train_type").eq("NRW").sum().cast(pl.Int64).alias("n_nrw"),
              pl.col("train_type").ne("NRW").sum().cast(pl.Int64).alias("n_non"),
          ])
    )

    if split_nrw_state:
        extra = (
            ev.filter(pl.col("train_type")=="NRW")
              .group_by(["Betriebstag","bst","t_bucket"])
              .agg([
                  pl.col("past_in_nrw").sum().cast(pl.Int64).alias("n_from"),
                  ((~pl.col("past_in_nrw")) & pl.col("future_in_nrw")).sum().cast(pl.Int64).alias("n_enter"),
              ])
        )
        counts = (base_counts.join(extra, on=["Betriebstag","bst","t_bucket"], how="left")
                            .with_columns([pl.col("n_from").fill_null(0), pl.col("n_enter").fill_null(0)]))
    else:
        counts = base_counts.with_columns([pl.lit(0).alias("n_from"), pl.lit(0).alias("n_enter")])

    # 3) Interaktionen pro Bucket (Kombinatorik)
    counts = counts.with_columns([
        # gleiche Typen
        (pl.col("n_nrw") * (pl.col("n_nrw") - 1) // 2).alias("c_NRWs"),
        (pl.col("n_non") * (pl.col("n_non") - 1) // 2).alias("c_NichtNRWs"),
        # gemischt (gesamt)
        (pl.col("n_nrw") * pl.col("n_non")).alias("c_gemischt"),
        # gemischt gesplittet (optional)
        (pl.col("n_from") * pl.col("n_non")).alias("c_gemischt_fromNRW"),
        (pl.col("n_enter") * pl.col("n_non")).alias("c_gemischt_enterLater"),
    ])

    # 4) Summe pro bst (über alle Tage & Buckets)
    agg_cols = [
        pl.sum("c_NRWs").alias("inter_NRWs"),
        pl.sum("c_NichtNRWs").alias("inter_NichtNRWs"),
        pl.sum("c_gemischt").alias("inter_gemischt"),
    ]
    if split_nrw_state:
        agg_cols += [
            pl.sum("c_gemischt_fromNRW").alias("inter_gemischt_fromNRW"),
            pl.sum("c_gemischt_enterLater").alias("inter_gemischt_enterLater"),
        ]

    per_bst = (
        counts.group_by("bst")
              .agg(agg_cols)
              .sort("bst")
    )

    return per_bst