import polars as pl

def interactions_nrw(
    df_zlm: pl.DataFrame,
    nrw_bsts: list[str],
    *,
    window_minutes: int = 10,
    mode: str = "bucket_combinatorics",   # "bucket_combinatorics" | "unique_per_day"
    split_nrw_state: bool = True,         # nur relevant f√ºr bucket_combinatorics
    group_by_day: bool = False,           # True ‚Üí je Betriebstag ausgeben
) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Einheitliche Interaktionsz√§hlung NRW vs. Nicht-NRW an Betriebsstellen.

    Spaltenvoraussetzungen:
      df_zlm muss enthalten: ["Betriebstag", "zn", "bst", "istzeit_dt"]
      nrw_bsts: Liste der NRW-Betriebsstellen

    mode:
      - "bucket_combinatorics": disjunkte Zeit-Buckets, kombinatorische Z√§hlung (zeitgewichtet)
      - "unique_per_day": jede Zugpaarung pro Tag/BST max. 1x gez√§hlt (dauerunabh√§ngig)

    R√ºckgabe:
      per_bst, debug_df
    """

    # 0Ô∏è‚É£ Grundstruktur + NRW-Markierung
    ev = (
        df_zlm
        .select(["Betriebstag", "zn", "bst", "istzeit_dt"])
        .with_columns(pl.col("bst").is_in(nrw_bsts).alias("in_nrw"))
        .sort(["Betriebstag", "zn", "istzeit_dt"])
    )

    # Zugtyp
    ev = ev.with_columns([
        pl.col("in_nrw").any().over(["Betriebstag","zn"]).alias("is_nrw_train"),
        pl.when(pl.col("in_nrw").any().over(["Betriebstag","zn"]))
          .then(pl.lit("NRW"))
          .otherwise(pl.lit("Nicht-NRW"))
          .alias("train_type"),
        pl.col("istzeit_dt").dt.truncate(f"{window_minutes}m").alias("t_bucket"),
    ])

    # üß© ----------------------------------------
    # MODE 1: BUCKET COMBINATORICS
    # ----------------------------------------
    if mode == "bucket_combinatorics":
        if split_nrw_state:
            ev = ev.with_columns([
                pl.col("in_nrw").cum_max().over(["Betriebstag","zn"]).shift(1).fill_null(False).alias("past_in_nrw"),
                pl.col("in_nrw").reverse().cum_max().reverse().over(["Betriebstag","zn"]).shift(-1).fill_null(False).alias("future_in_nrw"),
            ])

        base_counts = (
            ev.group_by(["Betriebstag","bst","t_bucket"])
              .agg([
                  pl.col("train_type").eq("NRW").sum().cast(pl.Int64).alias("n_nrw"),
                  pl.col("train_type").ne("NRW").sum().cast(pl.Int64).alias("n_non"),
              ])
        )

        if split_nrw_state:
            extra = (
                ev.filter(pl.col("train_type")=="NRW")
                  .group_by(["Betriebstag","bst","t_bucket"])
                  .agg([
                      pl.col("past_in_nrw").sum().cast(pl.Int64).alias("n_from"),
                      ((~pl.col("past_in_nrw")) & pl.col("future_in_nrw")).sum().cast(pl.Int64).alias("n_enter"),
                  ])
            )
            counts = (
                base_counts.join(extra, on=["Betriebstag","bst","t_bucket"], how="left")
                .with_columns([pl.col("n_from").fill_null(0), pl.col("n_enter").fill_null(0)])
            )
        else:
            counts = base_counts.with_columns([pl.lit(0).alias("n_from"), pl.lit(0).alias("n_enter")])

        counts = counts.with_columns([
            (pl.col("n_nrw") * (pl.col("n_nrw") - 1) // 2).alias("c_NRWs"),
            (pl.col("n_non") * (pl.col("n_non") - 1) // 2).alias("c_NichtNRWs"),
            (pl.col("n_nrw") * pl.col("n_non")).alias("c_gemischt"),
            (pl.col("n_from") * pl.col("n_non")).alias("c_gemischt_fromNRW"),
            (pl.col("n_enter") * pl.col("n_non")).alias("c_gemischt_enterLater"),
        ])

        group_keys = ["bst"] if not group_by_day else ["Betriebstag","bst"]
        agg_cols = [
            pl.sum("c_NRWs").alias("inter_NRWs"),
            pl.sum("c_NichtNRWs").alias("inter_NichtNRWs"),
            pl.sum("c_gemischt").alias("inter_gemischt"),
        ]
        if split_nrw_state:
            agg_cols += [
                pl.sum("c_gemischt_fromNRW").alias("inter_gemischt_fromNRW"),
                pl.sum("c_gemischt_enterLater").alias("inter_gemischt_enterLater"),
            ]

        per_bst = counts.group_by(group_keys).agg(agg_cols).sort(group_keys)
        debug_df = counts

        return per_bst, debug_df

    # üß© ----------------------------------------
    # MODE 2: UNIQUE PER DAY
    # ----------------------------------------
    elif mode == "unique_per_day":
        ev_binned = ev.select(["Betriebstag","zn","bst","t_bucket"]).unique()

        # Alle Paare innerhalb desselben Buckets
        pairs_bucket = (
            ev_binned.join(ev_binned, on=["Betriebstag","bst","t_bucket"], how="inner", suffix="_r")
                     .filter(pl.col("zn") < pl.col("zn_r"))  # jede Paarung nur einmal
                     .select(["Betriebstag","bst","zn","zn_r"])
        )

        # Einmal pro Tag/Station deduplizieren
        pairs_unique = pairs_bucket.unique(subset=["Betriebstag","bst","zn","zn_r"])

        # NRW-Z√ºge bestimmen
        nrw_trains = (
            ev.group_by(["Betriebstag","zn"])
              .agg(pl.col("in_nrw").any().alias("is_nrw"))
        )

        pairs_labeled = (
            pairs_unique
            .join(nrw_trains, on=["Betriebstag","zn"], how="left")
            .rename({"is_nrw":"is_nrw_left"})
            .join(nrw_trains, left_on=["Betriebstag","zn_r"], right_on=["Betriebstag","zn"], how="left")
            .rename({"is_nrw":"is_nrw_right"})
            .with_columns([
                (pl.col("is_nrw_left") & pl.col("is_nrw_right")).alias("pair_nrw_nrw"),
                (~pl.col("is_nrw_left") & ~pl.col("is_nrw_right")).alias("pair_non_non"),
                (pl.col("is_nrw_left") ^ pl.col("is_nrw_right")).alias("pair_mixed"),
            ])
        )

        # Aggregation
        group_keys = ["bst"] if not group_by_day else ["Betriebstag","bst"]

        per_bst = (
            pairs_labeled.group_by(group_keys)
              .agg([
                  pl.sum("pair_nrw_nrw").alias("inter_NRWs"),
                  pl.sum("pair_non_non").alias("inter_NichtNRWs"),
                  pl.sum("pair_mixed").alias("inter_gemischt"),
              ])
              .sort(group_keys)
        )

        debug_df = pairs_labeled
        return per_bst, debug_df

    else:
        raise ValueError("mode muss 'bucket_combinatorics' oder 'unique_per_day' sein.")


# Disjunkte Buckets, zeitgewichtete Interaktionen
per_bst_rate, debug_rate = interactions_nrw(
    df_zlm,
    nrw_bsts,
    window_minutes=10,
    mode="bucket_combinatorics",
    split_nrw_state=True,
    group_by_day=False,
)

# Eindeutige Begegnungen (jede Paarung max. 1x pro Tag & Station)
per_bst_unique, debug_unique = interactions_nrw(
    df_zlm,
    nrw_bsts,
    window_minutes=10,
    mode="unique_per_day",
    group_by_day=True,  # optional
)